# -*- coding: utf-8 -*-
"""spacy_tp3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I3LnB5syTqj0_3phTRXRucKrDKLOs-vD
"""

!python -m spacy download pt_core_news_sm #portuguese
!python -m spacy download zh_core_web_sm #chinese
!python -m spacy download sv_core_news_sm #swedish
!python -m spacy download hr_core_news_sm #croatian
!python -m spacy download en_core_web_sm #english
!python -m spacy download da_core_news_sm #danish
!python -m spacy download xx_ent_wiki_sm #multilanguage
import spacy
from spacy.training.example import Example
from spacy.scorer import Scorer
#from spacy.training.iob_utils import offsets_to_biluo_tags
import numpy as np
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.preprocessing import MultiLabelBinarizer

import json
import warnings
warnings.filterwarnings("ignore")

#convert spacy label to biolabel
def convert_to_bio_label(doc):
  bio_tags = ['O'] * len(doc)
  for ent in doc.ents:
    start, end, label = ent.start, ent.end - 1, ent.label_
    if end - start == 0:
      bio_tags[start] = f'B-{label}'
    else:
      bio_tags[start] = f'B-{label}'
      bio_tags[start+1 : end+1] = [f'I-{label}']*(end-start)
  return bio_tags

#Danish

nlp_da = spacy.load("da_core_news_sm")
test_file_da = open("da_ddt-ud-test.iob2", "r", encoding="utf-8")
iob2_test_data_da = test_file_da.read()
y_pred = []
y_true = []

for sentence in iob2_test_data_da.split("# sent_id"): #Every sentence begins with sentence id
    if not sentence.strip():
        continue

    y_true_sent = []
    y_pred_sent = []

    tokens_da = []
    entities_da = []
    for line in sentence.strip().split("\n"):
        parts = line.split("\t")
        if len(parts) == 5:
            part1, token, label, part4, part5 = parts
            tokens_da.append(token)
            entities_da.append((len(" ".join(tokens_da[:-1])) + 1, len(" ".join(tokens_da)), label))
            y_true_sent.append(label)

    example = Example.from_dict(nlp_da.make_doc(" ".join(tokens_da)), {"entities": entities_da})
    doc_da = spacy.tokens.Doc(nlp_da.vocab, words=tokens_da)
    processed_doc_da = nlp_da.get_pipe("ner")(doc_da)

    bio_labels_da = [token.ent_iob_ + "-" + token.ent_type_ for token in processed_doc_da]
    bio_labels_da = ["O" if l == "O-" else l for l in bio_labels_da]

    for dnb in list(zip(processed_doc_da, bio_labels_da)):
        #print(dnb)
        y_pred_sent.append(dnb[1])

    y_pred += y_pred_sent
    y_true += y_true_sent

print("Classification report for Danish")
print(classification_report(y_true,y_pred), spacy_report)

#English

nlp_en = spacy.load("en_core_web_sm")
test_file_en = open("en_ewt-ud-test.iob2", "r", encoding="utf-8")
iob2_test_data_en = test_file_en.read()
y_pred = []
y_true = []

for sentence in iob2_test_data_en.split("# sent_id"): #Every sentence begins with sentence id
    if not sentence.strip():
        continue

    y_true_sent = []
    y_pred_sent = []

    tokens_en = []
    entities_en = []
    for line in sentence.strip().split("\n"):
        parts = line.split("\t")
        if len(parts) == 5:
            part1, token, label, part4, part5 = parts
            tokens_en.append(token)
            entities_en.append((len(" ".join(tokens_en[:-1])) + 1, len(" ".join(tokens_en)), label))
            y_true_sent.append(label)

    example = Example.from_dict(nlp_en.make_doc(" ".join(tokens_en)), {"entities": entities_en})
    doc_en = spacy.tokens.Doc(nlp_en.vocab, words=tokens_en)
    processed_doc_en = nlp_en.get_pipe("ner")(doc_en)

    bio_labels_en = [token.ent_iob_ + "-" + token.ent_type_ for token in processed_doc_en]
    bio_labels_en = ["O" if l == "O-" else l for l in bio_labels_en]

    for dnb in list(zip(processed_doc_en, bio_labels_en)):
        #print(dnb)
        y_pred_sent.append(dnb[1])

    y_pred += y_pred_sent
    y_true += y_true_sent

print()
print("Classification report for English")
print(classification_report(y_true, y_pred))

#Croatian

nlp_hr = spacy.load("hr_core_news_sm")
test_file_hr = open("hr_set-ud-test.iob2", "r", encoding="utf-8")
iob2_test_data_hr = test_file_hr.read()
y_pred = []
y_true = []

for sentence in iob2_test_data_hr.split("# sent_id"): #Every sentence begins with sentence id
    if not sentence.strip():
        continue

    y_true_sent = []
    y_pred_sent = []

    tokens_hr = []
    entities_hr = []
    for line in sentence.strip().split("\n"):
        parts = line.split("\t")
        if len(parts) == 5:
            part1, token, label, part4, part5 = parts
            tokens_hr.append(token)
            entities_hr.append((len(" ".join(tokens_hr[:-1])) + 1, len(" ".join(tokens_hr)), label))
            y_true_sent.append(label)

    example = Example.from_dict(nlp_hr.make_doc(" ".join(tokens_hr)), {"entities": entities_hr})
    doc_hr = spacy.tokens.Doc(nlp_hr.vocab, words=tokens_hr)
    processed_doc_hr = nlp_hr.get_pipe("ner")(doc_hr)

    bio_labels_hr = [token.ent_iob_ + "-" + token.ent_type_ for token in processed_doc_hr]
    bio_labels_hr = ["O" if l == "O-" else l for l in bio_labels_hr]

    for dnb in list(zip(processed_doc_hr, bio_labels_hr)):
        #print(dnb)
        y_pred_sent.append(dnb[1])

    y_pred += y_pred_sent
    y_true += y_true_sent

print()
print("Classification report for Croatian")
print(classification_report(y_true, y_pred))

#Portuguese

nlp_pt = spacy.load("pt_core_news_sm")
test_file_pt = open("pt_bosque-ud-test.iob2", "r", encoding="utf-8")
iob2_test_data_pt = test_file_pt.read()
y_pred = []
y_true = []

for sentence in iob2_test_data_pt.split("# sent_id"):
    if not sentence.strip():
        continue

    y_true_sent = []
    y_pred_sent = []

    tokens_pt = []
    entities_pt = []
    for line in sentence.strip().split("\n"):
        parts = line.split("\t")
        if len(parts) == 5:
            part1, token, label, part4, part5 = parts
            tokens_pt.append(token)
            entities_pt.append((len(" ".join(tokens_pt[:-1])) + 1, len(" ".join(tokens_pt)), label))
            y_true_sent.append(label)

    example = Example.from_dict(nlp_pt.make_doc(" ".join(tokens_pt)), {"entities": entities_pt})
    doc_pt = spacy.tokens.Doc(nlp_pt.vocab, words=tokens_pt)
    processed_doc_pt = nlp_pt.get_pipe("ner")(doc_pt)

    bio_labels_pt = [token.ent_iob_ + "-" + token.ent_type_ for token in processed_doc_pt]
    bio_labels_pt = ["O" if l == "O-" else l for l in bio_labels_pt]

    for dnb in list(zip(processed_doc_pt, bio_labels_pt)):
        #print(dnb)
        y_pred_sent.append(dnb[1])

    y_pred += y_pred_sent
    y_true += y_true_sent

print()
print("Classification report for Portuguese")
print(classification_report(y_true, y_pred))

#Slovak

nlp_sk = spacy.load("xx_ent_wiki_sm")
test_file_sk = open("sk_snk-ud-test.iob2", "r", encoding="utf-8")
iob2_test_data_sk = test_file_sk.read()
y_pred = []
y_true = []

for sentence in iob2_test_data_sk.split("# sent_id"):
    if not sentence.strip():
        continue

    y_true_sent = []
    y_pred_sent = []

    tokens_sk = []
    entities_sk = []
    for line in sentence.strip().split("\n"):
        parts = line.split("\t")
        if len(parts) == 5:
            part1, token, label, part4, part5 = parts
            tokens_sk.append(token)
            entities_sk.append((len(" ".join(tokens_sk[:-1])) + 1, len(" ".join(tokens_sk)), label))
            y_true_sent.append(label)

    example = Example.from_dict(nlp_sk.make_doc(" ".join(tokens_sk)), {"entities": entities_sk})
    doc_sk = spacy.tokens.Doc(nlp_sk.vocab, words=tokens_sk)
    processed_doc_sk = nlp_sk.get_pipe("ner")(doc_sk)

    bio_labels_sk = [token.ent_iob_ + "-" + token.ent_type_ for token in processed_doc_sk]
    bio_labels_sk = ["O" if l == "O-" else l for l in bio_labels_sk]

    for dnb in list(zip(processed_doc_sk, bio_labels_sk)):
        #print(dnb)
        y_pred_sent.append(dnb[1])

    y_pred += y_pred_sent
    y_true += y_true_sent

print()
print("Classification report for Slovak")
print(classification_report(y_true, y_pred))

#Serbian

nlp_sr = spacy.load("xx_ent_wiki_sm")
test_file_sr = open("sr_set-ud-test.iob2", "r", encoding="utf-8")
iob2_test_data_sr = test_file_sr.read()
y_pred = []
y_true = []

for sentence in iob2_test_data_sr.split("# sent_id"):
    if not sentence.strip():
        continue

    y_true_sent = []
    y_pred_sent = []

    tokens_sr = []
    entities_sr = []
    for line in sentence.strip().split("\n"):
        parts = line.split("\t")
        if len(parts) == 5:
            part1, token, label, part4, part5 = parts
            tokens_sr.append(token)
            entities_sr.append((len(" ".join(tokens_sr[:-1])) + 1, len(" ".join(tokens_sr)), label))
            y_true_sent.append(label)

    example = Example.from_dict(nlp_sr.make_doc(" ".join(tokens_sr)), {"entities": entities_sr})
    doc_sr = spacy.tokens.Doc(nlp_sr.vocab, words=tokens_sr)
    processed_doc_sr = nlp_sr.get_pipe("ner")(doc_sr)

    bio_labels_sr = [token.ent_iob_ + "-" + token.ent_type_ for token in processed_doc_sr]
    bio_labels_sr = ["O" if l == "O-" else l for l in bio_labels_sr]

    for dnb in list(zip(processed_doc_sr, bio_labels_sr)):
        #print(dnb)
        y_pred_sent.append(dnb[1])

    y_pred += y_pred_sent
    y_true += y_true_sent

print()
print("Classification report for Serbian")
print(classification_report(y_true, y_pred))

#Swedish

nlp_sv = spacy.load("sv_core_news_sm")
test_file_sv = open("sv_talbanken-ud-test.iob2", "r", encoding="utf-8")
iob2_test_data_sv = test_file_sv.read()
y_pred = []
y_true = []

for sentence in iob2_test_data_sv.split("# sent_id"):
    if not sentence.strip():
        continue

    y_true_sent = []
    y_pred_sent = []

    tokens_sv = []
    entities_sv = []
    for line in sentence.strip().split("\n"):
        parts = line.split("\t")
        if len(parts) == 5:
            part1, token, label, part4, part5 = parts
            tokens_sv.append(token)
            entities_sv.append((len(" ".join(tokens_sv[:-1])) + 1, len(" ".join(tokens_sv)), label))
            y_true_sent.append(label)

    example = Example.from_dict(nlp_sv.make_doc(" ".join(tokens_sv)), {"entities": entities_sv})
    doc_sv = spacy.tokens.Doc(nlp_sv.vocab, words=tokens_sv)
    processed_doc_sv = nlp_sv.get_pipe("ner")(doc_sv)

    bio_labels_sv = [token.ent_iob_ + "-" + token.ent_type_ for token in processed_doc_sv]
    bio_labels_sv = ["O" if l == "O-" else l for l in bio_labels_sv]

    for dnb in list(zip(processed_doc_sv, bio_labels_sv)):
        #print(dnb)
        y_pred_sent.append(dnb[1])

    y_pred += y_pred_sent
    y_true += y_true_sent

print()
print("Classification report for Swedish")
print(classification_report(y_true, y_pred))

#Chinese

nlp_zh = spacy.load("zh_core_web_sm")
test_file_zh = open("zh_gsdsimp-ud-test.iob2", "r", encoding="utf-8")
iob2_test_data_zh = test_file_zh.read()
y_pred = []
y_true = []

for sentence in iob2_test_data_zh.split("# sent_id"):
    if not sentence.strip():
        continue

    y_true_sent = []
    y_pred_sent = []

    tokens_zh = []
    entities_zh = []
    for line in sentence.strip().split("\n"):
        parts = line.split("\t")
        if len(parts) == 5:
            part1, token, label, part4, part5 = parts
            tokens_zh.append(token)
            entities_zh.append((len(" ".join(tokens_zh[:-1])) + 1, len(" ".join(tokens_zh)), label))
            y_true_sent.append(label)

    example = Example.from_dict(nlp_zh.make_doc(" ".join(tokens_zh)), {"entities": entities_zh})
    doc_zh = spacy.tokens.Doc(nlp_zh.vocab, words=tokens_zh)
    processed_doc_zh = nlp_zh.get_pipe("ner")(doc_zh)

    bio_labels_zh = [token.ent_iob_ + "-" + token.ent_type_ for token in processed_doc_zh]
    bio_labels_zh = ["O" if l == "O-" else l for l in bio_labels_zh]

    for dnb in list(zip(processed_doc_zh, bio_labels_zh)):
        #print(dnb)
        y_pred_sent.append(dnb[1])

    y_pred += y_pred_sent
    y_true += y_true_sent

print()
print("Classification report for Chinese")
print(classification_report(y_true, y_pred))