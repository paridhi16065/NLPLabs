# -*- coding: utf-8 -*-
"""TP2trainingMyself.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T7QPJOepg7JeEPoEs3XIK7lggCWHz9SB
"""

import gensim
import gensim.downloader
import nltk
import numpy
import sklearn
from sklearn.decomposition import PCA
from matplotlib import pyplot
from gensim.models import Word2Vec
from urllib.request import urlopen
from gensim.utils import tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import pandas as pd
from gensim import matutils


nltk.download('wordnet')
nltk.download('stopwords')

corpusBookUrls = ["https://www.gutenberg.org/cache/epub/22566/pg22566.txt",
                  "https://www.gutenberg.org/cache/epub/5/pg5.txt",
                  "https://www.gutenberg.org/cache/epub/730/pg730.txt",
                  "https://www.gutenberg.org/cache/epub/329/pg329.txt",
                  "https://www.gutenberg.org/cache/epub/17366/pg17366.txt",
                  "https://www.gutenberg.org/cache/epub/51275/pg51275.txt",
                  "https://www.gutenberg.org/cache/epub/50750/pg50750.txt",
                  "https://www.gutenberg.org/cache/epub/20781/pg20781.txt",
                  "https://www.gutenberg.org/cache/epub/5123/pg5123.txt",
                  "https://www.gutenberg.org/cache/epub/308/pg308.txt",
                  "https://www.gutenberg.org/cache/epub/31604/pg31604.txt",
                  "https://www.gutenberg.org/cache/epub/36090/pg36090.txt",
                  "https://www.gutenberg.org/cache/epub/685/pg685.txt",
                  "https://www.gutenberg.org/cache/epub/28026/pg28026.txt",
                  "https://www.gutenberg.org/cache/epub/2591/pg2591.txt",
                  "https://www.gutenberg.org/cache/epub/66057/pg66057.txt",
                  "https://www.gutenberg.org/cache/epub/38326/pg38326.txt",
                  "https://www.gutenberg.org/cache/epub/34030/pg34030.txt",
                  "https://www.gutenberg.org/cache/epub/33183/pg33183.txt",
                  "https://www.gutenberg.org/cache/epub/61635/pg61635.txt",
                  "https://www.gutenberg.org/cache/epub/67351/pg67351.txt",
                  "https://www.gutenberg.org/cache/epub/2598/pg2598.txt",
                  "https://www.gutenberg.org/cache/epub/26368/pg26368.txt"]

corpusText = ""
for url in corpusBookUrls:
  corpusText+=urlopen(url).read().decode('utf-8')

nltk.download('punkt')
sentences = nltk.sent_tokenize(corpusText)

def preprocessSentence(sentence):
  #tokenise, lemmatise, remove stop words
  sentence = sentence.lower()
  tokens = list(tokenize(sentence))
  lemmatiser = WordNetLemmatizer()
  lemmas = tokens
  for i in range(len(tokens)):
    lemmas[i] = lemmatiser.lemmatize(tokens[i])
  output = []
  for lemma in lemmas:
    if lemma not in list(stopwords.words('english')):
      output.append(lemma)
  return(output)

sentenceSeries = pd.Series(sentences)
processedCorpus = sentenceSeries.apply(preprocessSentence)
processedCorpus = processedCorpus[processedCorpus.apply(len)>1]

model = Word2Vec(sentences = processedCorpus, workers = 1)

def getSentenceVector(sentence):
  words = sentence.lower().split()
  NoW = 0
  sentenceVector = numpy.zeros(100)
  for word in words:
    #print(word)
    if word in model.wv.index_to_key:
      sentenceVector = numpy.add(sentenceVector,model.wv[word])
      NoW+=1
  if NoW == 0:
    NoW = 1
  sentenceVector = numpy.divide(sentenceVector,NoW)
  #print (str(NoW) +":" + sentence)
  return(sentenceVector)

inputSentenceFile = open("/content/T_sent.txt","r") #T_sent filepath can be substituted here
inputSentences = inputSentenceFile.readlines()
for i in range(len(inputSentences)):
  for char in '`~!@#$%^&*()-_+={}[]\\|:;\'\"<>,.?/\t\n':
    inputSentences[i] = inputSentences[i].replace(char," ")
sentenceVectors=[]
for sentence in inputSentences:
  sentenceVectors.append(getSentenceVector(sentence))

pca = PCA(n_components=2)

graphPts = pca.fit_transform(sentenceVectors)

#closestSentence
def cosineSimilarity(v1,v2):
  denominator = numpy.linalg.norm(v1)* numpy.linalg.norm(v2)
  if denominator != 0:
    return(numpy.dot(v1, v2)/denominator)
  else:
    return (0)

sentencePairs = []
for i in range(len(inputSentences)):
  maxCosSimilarity = 0
  sentencePairForTarget = inputSentences[i]
  for j in range(len(inputSentences)):
    cosSimilarityij = cosineSimilarity(sentenceVectors[i],sentenceVectors[j])
    if cosSimilarityij>maxCosSimilarity and i!=j:
      maxCosSimilarity = cosSimilarityij
      sentencePairForTarget = inputSentences[j]
  sentencePairs.append((inputSentences[i],sentencePairForTarget))

pyplot.scatter(graphPts[:,0],graphPts[:,1])
for i,sentence in enumerate(inputSentences):
  pyplot.annotate(sentence.split()[0], xy=(graphPts[i, 0], graphPts[i, 1]))
pyplot.rcParams['figure.figsize'] =[30, 30]
pyplot.show()

sentence_pair_file = open("out2.2.txt",'w')
for sp in sentencePairs:
  sentence_pair_file.write(sp[0].split()[0]+"\t"+sp[1].split()[0]+"\n")
sentence_pair_file.close()

inputSentenceFile.close()