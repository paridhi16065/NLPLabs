# -*- coding: utf-8 -*-
"""finetune_tp4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iT4l3sjYPh36yzdymBHZpUbm19Bd_PKz
"""

from transformers import BertForTokenClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup

from torch.utils.data import DataLoader, TensorDataset
import torch

from torch.nn import CrossEntropyLoss

import sys

input_lang = sys.argv[1]

training_file_dict = {"Chinese": "zh_gsdsimp-ud-train.iob2",
                      "Croatian": "hr_set-ud-train.iob2",
                      "Danish": "da_ddt-ud-train.iob2",
                      "English": "en_ewt-ud-train.iob2",
                      "Portuguese": "pt_bosque-ud-train.iob2",
                      "Serbian": "sr_set-ud-train.iob2",
                      "Slovak": "sk_snk-ud-train.iob2",
                      "Swedish": "sv_talbanken-ud-train.iob2"}

testing_file_dict = {"Chinese": "zh_gsdsimp-ud-test.iob2",
                      "Croatian": "hr_set-ud-test.iob2",
                      "Danish": "da_ddt-ud-test.iob2",
                      "English": "en_ewt-ud-test.iob2",
                      "Portuguese": "pt_bosque-ud-test.iob2",
                      "Serbian": "sr_set-ud-test.iob2",
                      "Slovak": "sk_snk-ud-test.iob2",
                      "Swedish": "sv_talbanken-ud-test.iob2"}

pretrained_dataset_dict = {"Chinese": "bert-base-multilingual-uncased",
                      "Croatian": "bert-base-multilingual-uncased",
                      "Danish": "bert-base-multilingual-uncased",
                      "English": "bert-base-uncased",
                      "Portuguese": "bert-base-multilingual-uncased",
                      "Serbian": "bert-base-multilingual-uncased",
                      "Slovak": "bert-base-multilingual-uncased",
                      "Swedish": "bert-base-multilingual-uncased"}

baseline_report_dict = {"Chinese": """              precision    recall  f1-score   support

       B-ORG       0.71      0.43      0.53       129
       I-ORG       0.64      0.48      0.55       183
       B-LOC       0.82      0.69      0.75       429
       I-LOC       0.84      0.70      0.76       337
       B-PER       0.79      0.54      0.64       205
       I-PER       0.82      0.69      0.75       124

   micro avg       0.79      0.62      0.69      1407
   macro avg       0.77      0.59      0.66      1407
weighted avg       0.79      0.62      0.69      1407""",

                        "Croatian": """             precision    recall  f1-score   support

       B-ORG       0.80      0.74      0.77       414
       I-ORG       0.63      0.59      0.61       293
       B-LOC       0.91      0.90      0.91       597
       I-LOC       0.86      0.69      0.77       116
       B-PER       0.86      0.84      0.85       392
       I-PER       0.88      0.87      0.88       264

   micro avg       0.83      0.80      0.82      2076
   macro avg       0.82      0.77      0.80      2076
weighted avg       0.83      0.80      0.81      2076""",
                      "Danish": """                 precision    recall  f1-score   support

       B-ORG       0.83      0.49      0.62       172
       I-ORG       0.84      0.56      0.67        55
       B-LOC       0.68      0.78      0.73        90
       I-LOC       0.00      0.00      0.00         4
       B-PER       0.81      0.78      0.80       184
       I-PER       0.88      0.96      0.91       139

   micro avg       0.81      0.72      0.76       644
   macro avg       0.67      0.59      0.62       644
weighted avg       0.81      0.72      0.75       644""",
                      "English": """                precision    recall  f1-score   support

       B-ORG       0.78      0.44      0.56       322
       I-ORG       0.60      0.45      0.52       276
       B-LOC       0.76      0.72      0.74       317
       I-LOC       0.46      0.46      0.46        72
       B-PER       0.86      0.71      0.78       449
       I-PER       0.82      0.80      0.81       243

   micro avg       0.76      0.62      0.68      1679
   macro avg       0.71      0.60      0.64      1679
weighted avg       0.76      0.62      0.68      1679""",
                      "Portuguese": """             precision    recall  f1-score   support

       B-ORG       0.84      0.66      0.74       456
       I-ORG       0.76      0.57      0.65       430
       B-LOC       0.77      0.79      0.78       317
       I-LOC       0.67      0.67      0.67       181
       B-PER       0.84      0.79      0.81       442
       I-PER       0.88      0.92      0.90       320

   micro avg       0.80      0.73      0.76      2146
   macro avg       0.79      0.73      0.76      2146
weighted avg       0.80      0.73      0.76      2146""",
                      "Serbian": """                precision    recall  f1-score   support

       B-ORG       0.89      0.80      0.84       251
       I-ORG       0.75      0.64      0.69       177
       B-LOC       0.91      0.96      0.93       387
       I-LOC       0.91      0.78      0.84        55
       B-PER       0.84      0.86      0.85       209
       I-PER       0.89      0.91      0.90       144

   micro avg       0.87      0.85      0.86      1223
   macro avg       0.86      0.82      0.84      1223
weighted avg       0.87      0.85      0.86      1223""",
                      "Slovak": """                 precision    recall  f1-score   support

       B-ORG       0.38      0.06      0.10        50
       I-ORG       0.42      0.06      0.11        80
       B-LOC       0.77      0.58      0.66       326
       I-LOC       0.44      0.09      0.15        76
       B-PER       0.74      0.65      0.69       539
       I-PER       0.87      0.78      0.82       316

   micro avg       0.77      0.58      0.66      1387
   macro avg       0.60      0.37      0.42      1387
weighted avg       0.73      0.58      0.63      1387""",
                      "Swedish": """                precision    recall  f1-score   support

       B-ORG       0.71      0.43      0.53       129
       I-ORG       0.64      0.48      0.55       183
       B-LOC       0.82      0.69      0.75       429
       I-LOC       0.84      0.70      0.76       337
       B-PER       0.79      0.54      0.64       205
       I-PER       0.82      0.69      0.75       124

   micro avg       0.79      0.62      0.69      1407
   macro avg       0.77      0.59      0.66      1407
weighted avg       0.79      0.62      0.69      1407"""}

iob_training_file = training_file_dict[input_lang]
iob_testing_file = testing_file_dict[input_lang]
pretrained_dataset = pretrained_dataset_dict[input_lang]
baseline_report = baseline_report_dict[input_lang]

def convert_IOB2_to_BERT_data(filepath):
    training_data = []
    f = open(filepath, "r")
    sentence_started = False
    sentence_data = {"sentence": "s", "labels": []}

    for line in f.readlines():
        if line[2:6] == "text":
            sentence_started = True
            sentence_data["sentence"] = line[9:].strip()

        elif sentence_started and len(line.split("\t")) == 5:
            part1, word, label, part4, part5 = line.split("\t")
            sentence_data["labels"].append(label.strip())

        elif line[2:9] == "sent_id":
            sentence_started = False
            training_data.append(sentence_data)
            sentence_data = {"sentence": "s", "labels": []}

    f.close()
    return (training_data[1:])

def prepare_input_data(training_data):
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    tokenized_inputs = tokenizer(
        [example["sentence"] for example in training_data],
        padding=True,
        truncation=True,
        return_tensors="pt",
    )

    attention_masks = tokenized_inputs["attention_mask"]

    label_mapping = {"O": 0, "B-PER": 1, "I-PER": 2, "B-LOC": 3, "I-LOC": 4, "B-ORG": 5, "I-ORG": 6, "B-MISC": 7, "I-MISC": 8}

    max_labels = max(len(example["labels"]) for example in training_data)

    label_ids = []

    for i, example in enumerate(training_data):
        example_label_ids = [label_mapping.get(label, label_mapping["O"]) for label in example["labels"]]

        example_label_ids += [label_mapping["O"]] * (max_labels - len(example_label_ids))

        label_ids.append(example_label_ids)

    labels = torch.tensor(label_ids)

    return tokenized_inputs, labels, attention_masks

filepath = iob_training_file #changed for language
training_data = convert_IOB2_to_BERT_data(filepath)
tokenized_inputs, labels, attention_masks = prepare_input_data(training_data)

dataset = TensorDataset(tokenized_inputs["input_ids"], attention_masks, labels)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

model_name = pretrained_dataset #changed for language
model = BertForTokenClassification.from_pretrained(model_name, num_labels=9)
tokenizer = BertTokenizer.from_pretrained(model_name)
optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 20
num_training_steps = num_epochs * len(dataloader)
optimizer = AdamW(model.parameters(), lr=5e-5)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*num_training_steps, num_training_steps=num_training_steps)

from torch.nn.functional import pad

criterion = CrossEntropyLoss()

model.train()

total_iterations = 0

for epoch in range(2):
    for inputs, masks, labels in dataloader:
        max_length = max(inputs.size(-1) for inputs_batch in inputs)
        padded_inputs = pad(inputs, (0, max_length - inputs.size(-1)))
        padded_masks = pad(masks, (0, max_length - masks.size(-1)))
        padded_labels = pad(labels, (0, max_length - labels.size(-1)))

        outputs = model(padded_inputs, attention_mask=padded_masks, labels=padded_labels)
        loss = outputs.loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()
        #total_iterations += 1
        #print(total_iterations)
        if (total_iterations>200):
          break

test_filepath = iob_testing_file #changed for language
training_data = convert_IOB2_to_BERT_data(filepath)

tokenized_inputs, labels, attention_masks = prepare_input_data(training_data)

batch_size = 1
test_dataset = TensorDataset(tokenized_inputs["input_ids"], attention_masks, labels)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model.to(device)
model.eval()

all_predictions = []
all_labels = []

for test_batch in test_dataloader:
    inputs, masks, labels = test_batch

    inputs = inputs.to(device)
    masks = masks.to(device)
    labels = labels.to(device)

    #forward pass
    with torch.no_grad():
        outputs = model(inputs, attention_mask=masks)

    predictions = torch.argmax(outputs.logits, dim=-1)
    all_predictions.append(predictions)
    all_labels.append(labels)

all_predictions = torch.cat(all_predictions, dim=0)
all_labels = torch.cat(all_labels, dim=0)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import numpy as np

all_predictions = all_predictions.cpu().numpy().flatten()
all_labels = all_labels.cpu().numpy().flatten().flatten()


all_predictions = np.array(all_predictions)
all_labels = np.array(all_labels)
label_names = ["O", "B-PER", "I-PER", "B-LOC", "I-LOC", "B-ORG", "I-ORG", "B-MISC", "I-MISC"]

accuracy = accuracy_score(all_labels, all_predictions[:len(all_labels)])
conf_matrix = confusion_matrix(all_labels, all_predictions[:len(all_labels)])
report = classification_report(all_labels, all_predictions[:len(all_labels)], labels=[0,1,2,3,4,5,6], target_names=label_names)

import re

#changed for language
fine_tuned_report = report

def parse_report(report_str):
    lines = report_str.strip().split('\n')
    labels = []
    metrics = []

    for line in lines[2:-3]:
        parts = re.split(r'\s+', line.strip())
        label = parts[0]
        values = [float(x) if x != 'support' else int(parts[-1]) for x in parts[1:]]
        labels.append(label)
        metrics.append(values)

    return labels, metrics

baseline_labels, baseline_metrics = parse_report(baseline_report)
fine_tuned_labels, fine_tuned_metrics = parse_report(fine_tuned_report)

metric_diff = {}
diff_names = ["p_diff","r_diff", "f_diff", "s_diff"]
for label, baseline, fine_tuned in zip(baseline_labels, baseline_metrics, fine_tuned_metrics):
    min_len = min(len(baseline), len(fine_tuned))
    diff_values = {diff_names[i]: fine_tuned[i] - baseline[i] for i in range(min_len)}
    metric_diff[label] = diff_values

for label, diff in metric_diff.items():
    print(f"Differences for label {label}: {diff}")

print(report)